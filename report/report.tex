\input{preamble.tex}

\title{CI/CD with Jenkins and Beaker}
\author{Thomas LÃ¸kkeborg, 473157} % TODO add studnr

\begin{document}

\maketitle

\abstract{CI/CD is high-level term that is hard to grasp without practical examples. In this report I will put forward my own experiences from trying to set up a Jenkins CI/CD pipeline running in OpenStack using Beaker and Vagrant.}

\thispagestyle{empty}

\clearpage
\pagenumbering{roman}
\setcounter{page}{1}
\tableofcontents

\clearpage
\pagenumbering{arabic}

\section{Introduction}

The original goal of this project was to experiment with CI/CD using Jenkins and Beaker. As a vessel for achieving this goal I decided to try and create an infrastructure with a Puppet Master, a Jenkins server and an application server, where changes to the infrastructure were run through a CI/CD pipeline on the Jenkins server. The whole infrastructure should be completely defined in code, such that all that is needed to bring up a copy is some environment-dependant "yaml" files and a \mintinline{bash}{openstack create} command.
\\
\\
See \ref{listofwork} for a list of what I worked on during the project.

%This is a short template you can use for your project report. Feel free to
%make your own modifications to the structure. 
%
%Join together to form project groups of max three people in each group (you can
%also be just one if you prefer to work alone).
%
%For the topic you choose write a report of ca 5-15 pages plus appendices (and prepare a 5-10
%minute presentation).
%
%Try to write as scientifically correct as you can. Write objectively, do not
%write like you are telling a story: "first we did this, then we did that, ...".
%Use correct citations, here are some examples of how to cite correctly:
%\begin{itemize}
%\item Dag Langmyrs \LaTeX book~\cite{Langmyr:03}
%\item Journal articles like~\cite{Klein:09}
%\item Conference proceedings articles like~\cite{Begnum:07} 
%\item Wikipedia pages like~\cite{wikipedia:kerberos:10}
%\end{itemize}
%
%NOTE: it is EXTREMELY important that you write in your own words, and not
%just translate something you find. 
%
%If you want to write ``the perfect introduction'' I strongly suggest you
%read Claerbout's ``Scrutiny of the introduction''~\cite{Claerbout:95}.
%
%Describe what you will do in the introduction chapter (What are the goals of
%you project?).

\section{Background terms and technology}

\subsection{CI/CD}

As the whole project is focused around CI/CD I feel it's appropriate to present the definition of the term I've been using during the project. The term refers to the combined practise of "Continuous Integration" and "Continuous Delivery". Our course book\cite{coursebook} refers to Martin Fowler for definitions of these terms, so I will as well.

\subsubsection{Continuous Integration}

\say{Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.}\cite{cidef}

\subsubsection{Continuous Delivery}

\say{Continuous Delivery is a software development discipline where you build software in such a way that the software can be released to production at any time.}\cite{cddef}

\subsection{Beaker}

Beaker\cite{beakergithub} is a test harness focused on acceptance testing machines. The project is maintained by Puppetlabs themselves, and is widely used. Beaker abstracts away the underlying machines it is testing on by using the concept of SUT's, "Systems Under Test", so it can test for any kind of machine that is supported though plugins. The project consists of many parts, and the relevant ones for this project are "beaker-openstack"\cite{beakeropenstackgithub} and "beaker-vagrant"\cite{beakervagrantgithub}.

% NOT NEEDED?
%\subsubsection{beaker-rspec}

%Beaker-rspec incorporates the popular testing framework Rspec into the Beaker ecosystem. Rspec tests are 

\subsubsection{beaker-openstack}

Beaker-openstack is meant to allow Beaker to use OpenStack as a provider of SUT's. This plugin is experimental, and the library code reflects that. I have not found any examples of people actually using this plugin.

\subsubsection{beaker-vagrant}

Beaker-vagrant utilizes Vagrant to create and provision nodes. When using this plugin Beaker will create Vagrant configuration files for you, so it seems like there is no room for customization at the Vagrant layer.

\subsection{Vagrant}

Vagrant\cite{vagrantwebsite} is an Open-source automation tool for creating and provisioning virtual machines. It was created to make it straight-forward to create disposable and reproducible virtual machines defined in code. It is from my understanding primarily used to create development environments, but it can be used for anything involving virtual machines. Vagrant is not tied to any particular virtualisation technology, and uses the concept of "providers" to as a high-level abstraction for underlying hypervisors. As Vagrant is widely used you can find providers for pretty much anything. Worth mentioning is the VirtualBox and OpenStack providers, which are the ones I used during this project. The VirtualBox provider is the most popular choice, due to how simple it is to set up. There are a couple OpenStack providers, but I chose to use "vagrant-openstack-provider"\cite{vagrantopenstackprovidergithub}, as it bases it's machines on images from OpenStack, whereas the other one uses OpenStack compliant boxes.

\subsection{Jenkins Configuration as Code}

"Jenkins Configuration as Code"\cite{jcascwebsite}, or "JCasC", is a Jenkins plugin that seeks to replace the Jenkins web UI with yaml configuration files. Using this plugin it is possible to completely configure a Jenkins server through code. The project has been accepted as a standard component of the Jenkins project, and will be incorporated into Jenkins itself eventually. JCasC is very new, so documentation is sparse.

%Describe the technology involved, you do not have to explain something you have
%learned in the course, but explain any additional technology that you use in
%the prpoject.

\section{Survey of similar projects}

Following are the projects I looked at when trying to set up control-repo testing. Testing of my application is more of a "solved issue", so I did not look to any specific project for that.

\subsection{PSICK}

"Puppet Systems Infrastructure Construction Kit"\cite{psickgithub}, or "PSICK", is a feature-rich tool for generating Puppet control-repos. Control-repos generated by this tool come with many features included, and one of those are testing. The project uses Beaker for testing, and allows for testing on Docker containers or through Vagrant.
\\
\\
I did not choose to use this project as my control-repo because it seems rather complex, and because it seems to rely on either Vagrant with VirtualBox or Docker to run it's Beaker tests.

\subsection{Onceover}

Onceover\cite{onceovergithub} is a tool for generating and running tests for control-repos. It seems very promising, as it automatically generates tests by looking through your existing codebase. It used to support acceptance testing through Beaker, but the maintainers have abandoned Beaker to look for a more general solution to acceptance testing \cite{onceoverbeakerpr}. I did try and set this up for my project, but could not find a way to make it run on OpenStack.

\subsection{Acceptance testing at OpenStack}

OpenStack is using a combination of Vagrant and Beaker to run their Puppet module tests on OpenStack instances \cite{puppetmodulefunctionaltestingbeakerinsidevagrant}\cite{openstackpuppetmodulefunctionaltestingproposal}. Because the support for OpenStack nodes is so poor they have decided to create the SUT's via Vagrant using OpenStack as the provider, install Beaker as part of the provisioning process and run Beaker directly on the host (Vagrant controlled) machine by setting Beaker's hypervisor to "none" \cite{puppetmodulefunctionaltestingbeakerinsidevagrantnodeexample}\cite{openstackci}.

%\subsection{puppetlabs-ntp} for beaker-rspec stuff?

%There are always someone who has done what you are going to do, or
%something related. Breifly describe at least one or two similar projects,
%to motivate what your contribution will be (Google/DuckDuckGo the best you can
%and find similar projects).

\section{List of work} \label{listofwork}

Following is a description of what I spent my time working on during the project.

\subsection{Research}

During the project research was done so I had a general idea of how people where solving the issues I was working on. As a result of this I'm left with a better perspective on automated testing, the Jenkins ecosystem, Beaker and on Vagrant.

\subsection{The imt3005-project-cicd repository}

\url{https://github.com/tholok97/imt3005-project-cicd}
\\
\\
This repository serves as the control-repo for the infrastructure I built during the project, and also includes OpenStack Heat code to bring up a stack running the control-repo. During the project I manually tested features directly on the OpenStack instances of the stack, and tried to incorporate changes into this repository once they were stable enough.

\subsection{The imt3005-vagrant-vm repository}

\url{https://github.com/tholok97/imt3005-vagrant-vm}
\\
\\
This repository contains the Vagrant environment I used to develop for this project. It includes all the tooling needed to work with the course, and is written to be general enough that other people might find it useful. It is not the focus of this report, but I feel it is worth mentioning, as the bulk of my Vagrant experimentation ended up being setting up this environment.

\subsection{The gossinbackup repository}

\url{https://github.com/tholok97/gossinbackup}
\\
\\
Most of the Beaker documentation is about Puppet module testing, so as part of learning Beaker I wrote some basic acceptance tests for my gossinbackup module.

%Describe what you have done (or are doing), relate it the the previous work
%you described in the previous chapter if appropriate.

\section{Results and discussion}

\subsection{OpenStack Heat stack}

A core part of this project was having the infrastructure completely defined in code, so creating the stack was key. I based my infrastructure on IaC-Heat-cr\cite{iacheatcrgithub}, so creating the initial setup was went quite quickly. What I've needed to change from that stack was to reduce it down to three machines, make it use my own control-repo instead of contro-repo-cr\cite{controlrepocr} and to inject some ssh keys through the boot scripts ran at stack creation.
\\
\\
I decided to have the infrastructure definition in the same repository as the control-repo for simplicity. This proved to be beneficial as the infrastructure definition and control-repo always were in-sync. A side-effect is that r10k deploys the infrastructure definition together with the roles and profiles, which seems a little dirty. 

\subsection{Beaker}

Beaker proved to be more difficult to work with than expected. The initial goal was to have my whole control-repo tested through Beaker, but this proved to be a difficult task inside of Skyhigh. 

\subsubsection{Issues with the documentation}

While learning with Beaker I discovered several issues in the documentation. This seems strange, as the project appears to be well maintained. The biggest bug I found was in what might be the most important piece of documentation of all: the "getting started" tutorial \cite{beakertutorialwithproblem}. Following the tutorial step by step rewards you with a big error message following the final command, which is not very inviting. The bug was not easy to fix either. It turns out the tutorial uses functionality from Beaker version 3 which has been completely revised for Beaker 4. Installing Puppet on SUT's used to be a part of the core Beaker project, but has since been moved out into a separate plugin and revised. This meant that before I could even run a Beaker test I had to learn one of it's plugins.
\\
\\
I intend to create an issue on the Beaker issue tracker \cite{beakerissuetracker} as soon as I have the time. This will be my first real Open Source contribution, which is exciting.

\subsubsection{The Beaker-openstack plugin}

The most common way to run Beaker seems to be using Vagrant with VirtualBox. This was what I initially planned on doing, but it turns out Skyhigh instances do not support hardware virtualization. VirtualBox running on a system without hardware virtualization is not a pleasant experience, as virtual machines must be 32-bit, and creation and deletion will be very slow. I abandoned VirtualBox for what in principle is a much better solution anyways: using Skyhigh itself as the hypervisor for Beaker.
\\
\\
In concept Beaker using Skyhigh for it's SUT's is a good idea. You could run your functional tests on the exact same images as the production systems are using. You could even provision them the exact same way as the production systems as you have access to the boot scripts for the instances in the stack. The issue is that to do this you are dependant on a plugin, and although there is a Beaker-openstack plugin, it is experimental and does not seem stable. First of all it does not immediately seem to support OpenStack Keystone version 3, which is what Skyhigh seems to be using. Authenticating with Keystone version 2 gives 404 responses, and the top-level API call indicates it is deprecated. Looking into the code for the library it does mention Keystone version 3 though, but it is using it in a very strange way. The following snippet is from the part of the library that is causing issues \cite{beakeropenstackproblematicpart}:

\begin{minted}[firstnumber=48,fontsize=\tiny,linenos,breaklines]{Ruby}
# Keystone version 3 requires users and projects to be scoped
if @credentials[:openstack_auth_url].include?('/v3/')
  @credentials[:openstack_user_domain]    = @options[:openstack_user_domain] || 'Default'
  @credentials[:openstack_project_domain] = @options[:openstack_project_domain] || 'Default'
end

@compute_client ||= Fog::Compute.new(@credentials)
\end{minted}

Line 48 to 52 seems like a temporary solution, as it forces you to use "Default" domain when authenticating. The domain Skyhigh is expecting is "NTNU", so this might be part of the issue. Line 54 is where the library completely fails. There is a bug from inside the function call, which is a poorly documented call to another library, so I did not spend time debugging it.

\subsubsection{Using Vagrant for creation of SUT's for Beaker} \label{beakerinsidevagrantexplanation}

While researching alternate approaches to having Beaker work with OpenStack this was the one that seemed the most stable. Instead of having Beaker manage it's SUT's, you instead use Vagrant and it's excellent "vagrant-openstack-plugin" to manage OpenStack instances to test on, install Beaker on the instances, and trick Beaker to run it's tests on the machine it is installed on, which is the Vagrant-managed OpenStack instance \cite{puppetmodulefunctionaltestingbeakerinsidevagrant}\cite{openstackpuppetmodulefunctionaltestingproposal}. This lets you create exact copies of the production instances, with the only difference being the Beaker installation. Beaker tests can run as normal, but you are giving away the ability to test the system from the outside, as Beaker is inside the SUT calling itself with localhost. Beaker also has the very interesting ability to create multiple SUT's and do tests between them, which you are missing out on.
\\
\\
I did not end up implementing this in the project control-repo, but I have done enough manual testing to know that automating the process is possible. An example of how this looks like is provided as an appendix: \ref{beakerinsidevagrantexample}. With the Vagrantfile set up you would run \mintinline{bash}{vagrant up --debug} followed by \mintinline{bash}{vagrant destroy -f --debug} in a pipeline script. Beaker would run it's tests during \mintinline{bash}{vagrant up --debug}, and fail if anything goes wrong, which is what we want. Jenkins would catch this error and display all the output back to us so we can figure out what went wrong.

\subsubsection{Beaker-rspec tests for gossinbackup module}

To learn Beaker I created simple tests for my Puppet gossinbackup module. I created them to run as Vagrant with VirtualBox as a hypervisor, so they unfortunately can not be used in the project infrastructure. The code is included here: \ref{gossinbackuptests}.

\subsection{Jenkins}

Jenkins is what I spent the majority of my time working with.

\subsubsection{Jenkins Configuration as Code}

I wanted my control-repo to define my whole Jenkins setup, so I went with using the Jenkins Configuration as Code plugin for configuration. It is a very new project, so documentation is sparse. The idea behind it is to mirror the web UI setup with yaml code, but the mapping is not one-to-one, so trying to set it up without help is like shooting in the dark. After struggling with setting it up from scratch myself, I found a good demo\cite{jcascdemogithub} and based my setup around it. 
\\
\\
The plugin does have functionality to export yaml code based on a running Jenkins instance. This does not seem very well fleshed out yet, but when this is in place creating the configuration will be a much less painful process. Once JCasC matures I believe it will be massively useful, but currently using it is pretty cumbersome.
\\
\\
My "jenkins.yaml" file is included as an appendix: \ref{jenkinsyaml}. It sets up an admin user, configures the credentials plugin and adds a bunch of seed jobs.

\subsubsection{Job DSL}

I used the Job DSL plugin \cite{jobdslgithub} to define "seed jobs", which are jobs used to initialize larger jobs found elsewhere. All my testing code was written in Jenkinsfiles, which I'll discuss next. Here is an example of such a "seed job", taken from my JCasC configuration file:

\begin{minted}[fontsize=\tiny,linenos,breaklines]{yaml}
jobs:
(...)
  # sets up control-repo testing
  - script: |
      pipelineJob("control repo") {
                  definition {
                      cpsScm {
                          scm {
                              git {
                                  remote {
                                      github('tholok97/imt3005-project-cicd', 'ssh')
                                      credentials('tholok97_imt3005-project-cicd_deploy-key')
                                  }
                              }
                          }
                      }
                  }
      }
\end{minted}

Line 1 defines a list of jobs, and line 5 to 18 is a "seed job" that sets up a pipeline for the control-repo. The "credentials" call on line 12 refers to a private ssh key stored using the credentials plugin. See the security discussion.
\\
\\
An issue worth mentioning is how to invoke these jobs. Continuous Integration is based around integrating each change you make into the main trunk, so triggering a job run on each push to it's repository is desired. This is not possible in Skyhigh, as the floating IP addresses you are given are internal to NTNU's network. Therefore I've been manually triggering job runs during this project. I could also have had the jobs run on an interval, but as the course book\cite{coursebook} states this is not good enough. For proper CI you would set up webhooks between Github (or whatever version control host you use) and the Jenkins server. 

\subsection{Test pipelines}

Each pipeline has a "seed job" that simply adds the job and makes the "Jenkinsfile" found in each repository do the rest. 

\subsubsection{Control-repo}

The control-repo's pipeline currently just does linting tests and deploys to production. With more time I would have written proper Beaker tests for it using the method described here: \ref{beakerinsidevagrantexplanation}. The following code snippet describes it's Jenkinsfile, with the parts I did not have time to write added:

% TODO.. read above

\begin{minted}[fontsize=\tiny,linenos,breaklines]{groovy}
pipeline {
  agent none
  stages {
    stage('puppet parser validate') {
      agent {
        // using dockerfile defined in jenkins_agents
        dockerfile {
          dir 'jenkins_agents/syntax_agent/'
        }
      }
      steps {
        sh '/opt/puppetlabs/bin/puppet parser validate --debug --verbose .'
      }
    }
    stage('puppet-lint') {
      agent {
        // using dockerfile defined in jenkins_agents
        dockerfile {
          dir 'jenkins_agents/syntax_agent/'
        }
      }
      steps {
        sh '/usr/bin/puppet-lint --error-level all --fail-on-warnings .'
      }
    }
    
    // BEGIN NOT IMPLEMENTED. Added for demonstration
    stage() {
      agent {
        // using dockerfile defined in jenkins_agents
        dockerfile {
          dir 'jenkins_agents/vagrant_agent/'
        }
      }
      steps {
        sh './runVagrantBeakerTest.sh'
      }
    }    
    // END NOT IMPLEMENTED. Rest is present in control-repo

    stage('r10k deploy') {
      agent { label 'master' }
      steps {
        sh "ssh -o StrictHostKeyChecking=no root@manager.borg.trek 'r10k deploy environment -p --verbose'"
      }
    }
  }
}
\end{minted}

% TODO: actually write that section

\subsubsection{gossinbackup}

While learning Beaker I set up basic acceptance tests for my gossinbackup module. These run fine, but use the Vagrant and VirtualBox solution, so I can not run them with my current setup on Skyhigh. Here is what the pipeline currently looks like, with the parts I did not get to implement added. The syntax checks are the same as the ones we did in lab task 7.

% TODO refer to appendix

\begin{minted}[fontsize=\tiny,linenos,breaklines]{groovy}
pipeline {
  agent {
    dockerfile {
      filename 'pdk_agent_dockerfile'
      dir 'jenkins_agents'
    }
  }
  stages {
    stage('Validate and lint') {
      steps {
        sh 'pdk validate metadata,puppet'
      }
    }
    stage('Unit tests') {
      steps {
        sh 'pdk test unit --debug'
      }
    }

    // BEGIN NOT IMPLEMENTED
    stage('Acceptance test') {
      steps {
        sh 'bundle install && bundle exec rspec spec/acceptance/'
        // OR
        sh './runVagrantBeakerTest.sh'
      }
    }
    // END NOT IMPLEMENTED
  }
}
\end{minted}

As indicated in the snippet, the acceptance tests could either run as standard beaker test runs, or by using the Vagrant solution I explain here: \ref{beakerinsidevagrantexample}. The first solution works, but not in Skyhigh, and the latter is not implemented.

\subsubsection{application} \label{applicationpipeline}

Because of time-constraints I did not get to implement any kind of CI/CD application pipeline. I did look into how I would go forward with doing it though, and set up a pipeline through the web UI that ultimately deployed a Golang\cite{golang} binary to the application server following a tutorial\cite{buidonjenkinsandpublishwithpipelines}. Here are the steps I would take to implement this in the control-repo:

\begin{enumerate}
\item Write a basic Golang application with a few tests. This is \textbf{trivial} to do in Golang.
\item Set up a seed job for the repository, following the same convention used for the control-repo and the gossinbackup module.
\item Set up a Jenkinsfile in the repository, matching the job created in the tutorial I followed \cite{buidonjenkinsandpublishwithpipelines}.
\item Deploy to the application server using the same basic ssh method used in the control-repo pipeline.
\end{enumerate}

\subsection{imt3005-vagrant-vm}

Although this was not the focus of this project, I will briefly discuss the Vagrant environment I created to work with this course \cite{tholok97vagrantvm}. Currently it includes all the tools needed, and I believe it is general enough that anyone can clone it and start using it themselves. Environment-specific configuration is done through a yaml file, and all the "data" of the environment is shared with the underlying host, so the environment is completely disposable. It gave me a chance to experiment freely, as I could simply rebuild it any time I messed up.
\\
\\
Relevant to this project is the fact that since I've done all of my work inside this environment, it should be possible for anyone to clone it down and get the exact same setup that I've used. This one solution to the "it works on my machine" issue, as everyone will have the exact same machine. In addition it's fully possible to make this environment mirror that found in production. I have also managed to successfully set up Vagrant to provision it's VM in Skyhigh, which allows it to use the exact same image as production does. You could take this even further by having Vagrant apply the same boot script to it as well.

\subsection{Jenkins Docker agents}

Jenkins can run it's tests within agents, and with the Docker plugin installed these agents can be Docker containers. I found this very useful as each repository can define it's own agent along with it's Jenkinsfile. This way only the seed job is defined by Jenkins itself, with the rest being defined in the repository the tests concern. For the control-repo I created one such agent \cite{imt3005projectcicd}, which has puppet agent and puppet-lint installed to be able to run \mintinline{bash}{puppet parser validate} and \mintinline{bash}{puppet-lint}. If I later decide to add other tests, say a yaml linting tool, I don't have to touch Jenkins itself, only code within the control-repo.
\\
\\
I have Jenkins running in a Docker container, and to avoid "Docker-in-Docker" problems I have it connected to the underlying Docker host for creation of containers. \cite{dockerindockerarticle}
\\
\\
I decided on a directory structure where additional agents can be added with ease. For example the control-repo would need an agent with Vagrant installed for the acceptance testing method I discussed here: \ref{beakerinsidevagrantexplanation}, and the application (discussed here: \ref{applicationpipeline}) would have an agent with Golang tools installed. See my control-repo \cite{imt3005projectcicd} for an example of this directory structure.



% WHAT TO TALK ABOUT
% - Stack
% - Beaker
%   - configuration
%   - 
% - JCasC
%   - configuration
%   - 
% tests:
%   - control-repo
%   - control-repo
%   - gossinbackup



%This is the longest chapter, feel free to include some figures. Include
%discussion of all problems you have encountered.

\section{Security aspects}

Security was not a primary focus of this project, so there are a few concerns that have not been fixed. I will discuss them here:

\subsection{Secrets}

Handling secrets was not a focus during this project, so I have chosen some dodgy solutions here and there. The only way of safely bringing in secrets with my setup is during provisioning of the Heat stack, by injecting them into the machines from Heat variables. The ssh key pair between the Jenkins server and the Puppet master is injected in this way, as well as the Github ssh key the Puppet master uses for r10k deploys. With access to my development machine an attacker would have access to every secret in my infrastructure, which I would consider a huge security issue for larger deployments. A solution here would be to generate the key pair between the Puppet master and Jenkins server either during stack creation or through Puppet.
\\
\\
JCasC lets you define user passwords in plaintext in the yaml configuration file, so I chose to do that for simplicity. This is an obvious security flaw that could be fixed by the below list of solutions.
\\
\\
The above problems are way smaller than the greatest sin I did during this project though, which was to include a private key in plaintext in the control-repo. The Jenkins server needs to authenticate with Github to clone down repositories for testing, and I did not find any straight-forward way to give Jenkins a private key for this, so I ended up just including the key as part it's JCasC configuration. The key is a "Github deploy key"\cite{githubdeploykeyarticle}, however, so an attacker with access to this key would only have \textbf{read} access to \textbf{the repository the key was linked to}. There are a couple solutions to this issue:
\\
\\
\begin{itemize}  
\item \textbf{Using JCasC secrets}: JCasC includes some basic functionality for handling secrets\cite{jcascgithub}. One is injecting secrets into the configuration from environment variables. I could have set up environment variables during stack creation and included the deploy key this way
\item \textbf{Using hiera-eyaml}: This would allow the key to be stored in the repository in an encrypted form. A decryption key could be given to the Puppet master during stack creation.
\item \textbf{Using an external secrets technology}: There lots of technologies for keeping secrets out of version control altogether. I did not look into any of them.
\end{itemize}

\subsection{SSH keys}

Currently the Puppet master has ssh access to both of the other machines. This is for more easily debugging the infrastructure, but does mean that access to the Puppet master means access to everything. The Jenkins server also has ssh access back to the Puppet master. This is because it runs \mintinline{bash}{r10k deploy} through ssh to deploy after testing. This was done for simplicity. A more secure solution would be to set up an API endpoint on the Puppet master that runs \mintinline{bash}{r10k deploy} itself when it receives request. That way access to the Jenkins machine only means an attacker could spam deploys, which is much better than an attacker having full access to the Puppet master through ssh. A simple socat one-liner \cite{socatwebhook} could do this.

\subsection{Concerns around defining Jenkins as Code}

Keeping all test definitions under version control means that an attacker with access to my repositories could see everything I test for, and more importantly, everything I \textbf{don't} test for. This is a consequence of having everything defined as code, and makes securing the repositories even more important. This issue is somewhat mitigated by the fact that the control-repo only contains "seed jobs". The actual test definitions are in each individual repository in the form of a "Jenkinsfile".

\subsection{Infrastructure defined in control-repo}

I chose to have the infrastructure definition in the control-repo, which means r10k "deploys" this code as well to the Puppet master. This means that with access to the Puppet master an attacker could find security holes in my setup. 

%Provide a brief security analysis. Are you opening any new attack vectors? What
%are the risks? Are there sensitive data? Are clients and servers mutually
%authenticating each other? etc etc

\section{Conclusions}

My original goal was to experiment with CI/CD, and I feel I have accomplished that. I am left with a technical understanding of how CI/CD could be implemented using the technologies I have explored. Continuous Integration in my project exists in the form of the pipeline jobs running tests on changes made (although running them automatically was not possible, as I explained). With the features I explored, but didn't implement, I would feel pretty safe inviting others to work on the codebase as well. Continuous Delivery is somewhat achieved by the fact that I steered away from experimenting though the CI/CD pipelines, but instead did rough manual experimentation and testing before committing to production.
\\
\\
Beaker turned out to be difficult to work with in Skyhigh. Most examples are of using Beaker with Vagrant and VirtualBox, which is not possible in Skyhigh, so most of the time spent on Beaker was getting it to even run. The method I arrived at does work though, although it has it's drawbacks, and I did not have time to fully implement it.
\\
\\
As for the infrastructure I wanted to set up, I am not finished. I am convinced I would be able to do it with more time however, and I have tried to lay out how I would achieve this in this report. What is implemented does meet my \mintinline{bash}{openstack create} requirement though. Following this requirement has slowed me down quite a bit, but in return I have a greater understanding and respect for how difficult it can be to define everything in code, especially with regards to secrets.

%* Didn't quite reach my goal
%* Working with Beaker outside of VirtualBox is a hassle
%* JCasC is very promising, but not very useable
%* Vagrant is great
%* Testing pipelines are great


%So what is the punchline, does it work or not? Do you recommend doing this
%for others?

\clearpage % make references start on own page

\nocite{*}
\bibliographystyle{acmdoi}
\bibliography{report}

\clearpage % make appendixstart on own page
\appendix

\section{Beaker inside Vagrant example} \label{beakerinsidevagrantexample}

\begin{minted}[fontsize=\tiny,linenos,breaklines]{ruby}
# -*- mode: ruby -*-
# vi: set ft=ruby :

require 'vagrant-openstack-provider'

#
# This is quite the minimal configuration necessary
# to start an OpenStack instance using Vagrant on
# an OpenStack with Keystone v3 API
#
# NOTE: this example is heavily
# inspired by http://my1.fr/blog/puppet-module-functional-testing-with-vagrant-openstack-and-beaker/
#
Vagrant.configure('2') do |config|

  config.ssh.username = 'ubuntu'

  config.vm.provider :openstack do |os, ov|
    os.server_name                      = 'vagrant_machine_in_openstack'
    os.security_groups                  = [ 'default', 'linux' ]
    os.identity_api_version             = '3'
    os.openstack_auth_url               = 'https://api.skyhigh.iik.ntnu.no:5000/v3'
    os.project_name                     = '<PROJECTNAME>'
    os.user_domain_name                 = 'NTNU'
    os.project_domain_name              = 'NTNU'
    os.username                         = '<USERNAME>'
    os.password                         = '<PASSWORD>'
    os.region                           = 'SkyHiGh'
    os.floating_ip_pool                 = 'ntnu-internal'
    os.floating_ip_pool_always_allocate = true
    os.flavor                           = 'm1.small'
    os.image                            = 'Ubuntu Server 16.04 LTS (Xenial Xerus) amd64'
    os.networks                         = [ '<INTERNALNETID>' ]

    ov.nfs.functional = false
  end

  # you could provision this machine using the same provisioning scripts used by 
  # Heat, to create an exact duplicate
  config.vm.provision "shell", path: "bootscriptFromHeat.sh"
  

  # shell to install beaker, setup ssh, and run beaker tests.
  # written inline for sake of example
  config.vm.provision "shell", inline: <<-SHELL
    #!/bin/bash

    # install deps
    sudo apt-get update
    sudo apt-get install -y libxml2-dev libxslt-dev zlib1g-dev git ruby ruby-dev build-essential

    # prepare ssh
    echo "" | sudo tee -a /etc/ssh/sshd_config
    echo "Match address 127.0.0.1" | sudo tee -a /etc/ssh/sshd_config
    echo "    PermitRootLogin without-password" | sudo tee -a /etc/ssh/sshd_config
    echo "" | sudo tee -a /etc/ssh/sshd_config
    echo "Match address ::1" | sudo tee -a /etc/ssh/sshd_config
    echo "    PermitRootLogin without-password" | sudo tee -a /etc/ssh/sshd_config
    mkdir -p .ssh
    ssh-keygen -f ~/.ssh/id_rsa -b 2048 -C "beaker key" -P ""
    sudo mkdir -p /root/.ssh
    sudo rm /root/.ssh/authorized_keys
    cat ~/.ssh/id_rsa.pub | sudo tee -a /root/.ssh/authorized_keys
    sudo service ssh restart
   
    # prepare gems
    # this uses my gossinbacup module as an example, but it would be
    # possible to have the module as a parameter to this process
    git clone https://github.com/tholok97/gossinbackup
    cd gossinbackup
    sudo gem install bundler --no-rdoc --no-ri --verbose
    bundle install

    # run tests
    # this relies on SUT yaml definitions with hyporvisor set to "none",
    # like here: https://github.com/openstack/puppet-keystone/blob/master/spec/acceptance/nodesets/nodepool-xenial.yml
    export BEAKER_debug=yes
    bundle exec rspec spec/acceptance
SHELL

end
\end{minted}

\section{Beaker tests for my gossinbackup module} \label{gossinbackuptests}

\subsection{spec\_helper\_acceptance.rb}

\begin{minted}[fontsize=\tiny,linenos,breaklines]{ruby}
require 'beaker-pe'
require 'beaker-puppet'
require 'beaker-rspec'
require 'beaker/puppet_install_helper'
require 'beaker/module_install_helper'

# use helpers to prepare the hosts with puppet, this
# module and it's dependencies
run_puppet_install_helper
configure_type_defaults_on(hosts)
install_module_on(hosts)
install_module_dependencies_on(hosts)

RSpec.configure do |c|
  c.formatter = :documentation

  c.before :suite do
    # Install module to all hosts
    hosts.each do |host|
      # this module uses git as a provider. Installing git on the hosts
      install_module_from_forge('puppetlabs-git', '0.5.0')
      apply_manifest_on(host, 'include git', catch_failures: true)
    end
  end
end
\end{minted}

\subsection{class\_spec.rb}

\begin{minted}[fontsize=\tiny,linenos,breaklines]{ruby}
require 'spec_helper_acceptance'

describe 'gossinbackup module' do
  let(:pp) { 'class { "gossinbackup": source_directory => "/home", destination_directory => "/backups", }' }

  context 'when manifest is applied' do
    it 'gives no errors' do
      apply_manifest(pp, catch_failures: true) do |r|
        expect(r.stderr).not_to match(%r{error}i)
        expect(r.exit_code).to eq(2)
      end
    end
  end

  context 'when manifest is applied a second time' do
    it 'does not change anything (is idempotent)' do
      apply_manifest(pp, catch_failures: true) do |r|
        expect(r.stderr).not_to match(%r{error}i)
        expect(r.exit_code).to be_zero
      end
    end
  end
end
\end{minted}

\section{Jenkins.yaml} \label{jenkinsyaml}

\begin{minted}[fontsize=\tiny,linenos,breaklines]{ruby}
# JCasC configuration file. Heavily inspired by https://github.com/Praqma/praqma-jenkins-casc
jenkins:
  systemMessage: "Greetings friend! I am a system message configured through JCasC"
  agentProtocols:
    - "JNLP4-connect"
  securityRealm:
    local:
      allowsSignup: false
      users:
       - id: insecureAdmin
         password: insecurePassword
  authorizationStrategy:
    globalMatrix:
      grantedPermissions:
        - "Overall/Read:anonymous"
        - "Job/Read:anonymous"
        - "View/Read:anonymous"
        - "Overall/Administer:authenticated"
  crumbIssuer: "standard"
credentials:
  system:
    domainCredentials:
      - credentials:
          - basicSSHUserPrivateKey:
              scope: GLOBAL  # this has to be global for scm to work in seed jobs
              id: tholok97_imt3005-project-cicd_deploy-key
              username: tholok97
              passphrase:  ""
              description: "ssh private key used to connect ssh slaves"
              privateKeySource:
                directEntry:
                  # YES. Including a private key in plaintext in VCS is horrible, but did it
                  # for simplicity. See my report for a discussion for possible solutions
                  #
                  # Note that is a Github deploy key, so it only has READ access to ONE
                  # repository, which I'll make public eventually anyways.
                  privateKey: |
                    -----BEGIN RSA PRIVATE KEY-----
                    <github deploy key was stored here. left out for brevity. see 
                    security discussion>
                    -----END RSA PRIVATE KEY-----

jobs:
# taken from https://github.com/jenkinsci/configuration-as-code-plugin/blob/master/docs/seed-jobs.md
# and https://marcesher.com/2016/08/04/jenkins-as-code-comparing-job-dsl-and-pipelines/
#
# The following three jobs are just here to show some examples of job dsl configured through JCasC
  - script: |
      def myJob = freeStyleJob('SimpleJob')
      myJob.with {
          description 'A Simple Job'
      }
  - script: |
      job('example-job-from-job-dsl') {
          scm {
              github('jenkinsci/job-dsl-plugin', 'master')
          }
          triggers {
              cron("@hourly")
          }
          steps {
              shell("echo 'Hello World'")
          }
      }
  - script: |
      pipelineJob("pipeline-calls-other-pipeline") {
                  logRotator{
                      numToKeep 30
                  }
                  definition {
                      cps {
                          sandbox()
                          script("""
                              node {
                                  echo 'Hello World 1'
                              }
                          """.stripIndent())
                          }
                      }
                  }
# The below job sets up a pipeline job for my gossinbackup module. The Jenkinsfile of that module is used.
# Added as a demonstration that jobs work
# Reference used: https://jenkinsci.github.io/job-dsl-plugin/#path/pipelineJob
  - script: |
      pipelineJob("pipeline-job-to-run-gossinbackup") {
                  definition {
                      cpsScm {
                          scm {
                              git('https://github.com/tholok97/gossinbackup', 'master')
                          }
                      }
                  }
      }
# sets up control-repo testing
  - script: |
      pipelineJob("control repo") {
                  definition {
                      cpsScm {
                          scm {
                              git {
                                  remote {
                                      github('tholok97/imt3005-project-cicd', 'ssh')
                                      credentials('tholok97_imt3005-project-cicd_deploy-key')
                                  }
                              }
                          }
                      }
                  }
      }
# These jobs are from the demo I copied this config from. Left in for reference
#  - url: https://raw.githubusercontent.com/Praqma/job-dsl-collection/master/configuration-as-code-dsl/pipeline.dsl #casc
#  - url: https://raw.githubusercontent.com/Praqma/memory-map-plugin/master/jenkins-pipeline/pipeline.groovy #memory map
#  - url: https://raw.githubusercontent.com/Praqma/codesonar-plugin/master/jenkins-pipeline/pipeline.groovy #codesonar
#  - url: https://raw.githubusercontent.com/Praqma/pretested-integration-plugin/master/jenkins-pipeline/pipeline.groovy #pretested integration
#  - url: https://raw.githubusercontent.com/Praqma/ClearCaseUCMPlugin/master/jenkins-pipeline/pipeline.groovy #ccucm
#  - url: https://raw.githubusercontent.com/Praqma/Praqmatic-Automated-Changelog/master/jenkins-pipeline/pipeline.groovy #pac
#  - url: https://raw.githubusercontent.com/Praqma/PlusBump/rebirth/jenkins-pipeline/pipeline.groovy #plusbump
#  - url: https://raw.githubusercontent.com/Praqma/job-dsl-collection/master/web-pipeline-dsl/web_pipeline_dsl.groovy #websites
#  - file: ./jobdsl/rut.groovy #rut (private repo...better keep it here)

tool:
  git:
    installations:
      - name: Default
        home: "git"

security:
  remotingCLI:
    enabled: false
\end{minted}

\end{document}
